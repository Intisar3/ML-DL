{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXYBQvxf4XdT",
        "outputId": "470be61d-4fef-40a9-86aa-3bda1c279569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.9)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA7xYAmd9GN3",
        "outputId": "7824a1c7-6922-4395-e143-4fd05dcfafd0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "rSUHm8oI9X-R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "OxD8MNNeahye"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "4YirJ6j5bfat"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7-zLqd7biHx",
        "outputId": "c67327c5-b64f-4ac8-9d92-87170d10fd18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
            "License(s): other\n",
            "Downloading chest-xray-pneumonia.zip to /content\n",
            " 99% 2.28G/2.29G [00:21<00:00, 223MB/s]\n",
            "100% 2.29G/2.29G [00:21<00:00, 114MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q chest-xray-pneumonia.zip -d /content/chest_xray\n",
        "print(\"Dataset extracted to /content/chest_xray\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ0hOtzTkIHn",
        "outputId": "508c0660-d0b2-49ad-d063-0d269807a41c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted to /content/chest_xray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2WaYdK7r-UC",
        "outputId": "69ab13b0-c8fe-4d0a-d165-c8076adf6f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Continue from where you stopped\n",
        "# Dataset has been downloaded and extracted to: /content/chest_xray\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50, InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import shutil\n",
        "import random\n"
      ],
      "metadata": {
        "id": "nhHG75YiFRu4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "base_path = '/content/chest_xray/chest_xray'\n",
        "train_dir = os.path.join(base_path, 'train')\n",
        "val_dir = os.path.join(base_path, 'val')\n",
        "test_dir = os.path.join(base_path, 'test')"
      ],
      "metadata": {
        "id": "8Ozcz8Dzlxrg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Efficient Data Preparation for Stage 1 (Normal vs Pneumonia)\n",
        "# Optimized for lower computation with comprehensive augmentation\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# ‚ö° Optimized Configuration\n",
        "img_size = 192  # Reduced from 224 for 30% less computation\n",
        "batch_size = 64  # Increased for better GPU utilization\n",
        "\n",
        "# üîß Enable Mixed Precision for 50% faster training\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# üéØ Comprehensive Data Augmentation - All Angles & Rotations\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=360,  # Full rotation coverage\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=[0.7, 1.3],\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.15,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='reflect'\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# üìä Generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# üöÄ Additional tf.data Optimizations\n",
        "def optimize_dataset(generator):\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, img_size, img_size, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
        "        )\n",
        "    )\n",
        "    return dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Apply optimizations\n",
        "train_dataset = optimize_dataset(train_generator)\n",
        "val_dataset = optimize_dataset(val_generator)\n",
        "test_dataset = optimize_dataset(test_generator)\n",
        "\n",
        "# üéØ Performance Optimizations\n",
        "tf.config.optimizer.set_jit(True)  # Enable XLA\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "\n",
        "print(f\"‚úÖ Optimized: {img_size}x{img_size}, batch={batch_size}, 360¬∞ rotation, mixed precision\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3rUA-SbWZTs",
        "outputId": "a4d86105-a031-4e1e-c405-a523eba5fe35"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "‚úÖ Optimized: 192x192, batch=64, 360¬∞ rotation, mixed precision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Stage 1 Model: ResNet50 for Normal vs Pneumonia\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "base_model_1 = ResNet50(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
        "\n",
        "x = base_model_1.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output_1 = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_stage1 = Model(inputs=base_model_1.input, outputs=output_1)"
      ],
      "metadata": {
        "id": "sCfdRFwKnT0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0693bd3e-d1f1-4471-aa73-ecebaeb349de"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze base layers\n",
        "for layer in base_model_1.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_stage1.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "]"
      ],
      "metadata": {
        "id": "fiA2-76tnc6p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Stage 1 model\n",
        "history_stage1 = model_stage1.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0j2t67nnifh",
        "outputId": "e22afd42-686a-472a-dd90-016a6f6c5e66"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1224s\u001b[0m 15s/step - accuracy: 0.7572 - loss: 0.5114 - val_accuracy: 0.5625 - val_loss: 0.6747\n",
            "Epoch 2/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1205s\u001b[0m 15s/step - accuracy: 0.7502 - loss: 0.5223 - val_accuracy: 0.5625 - val_loss: 0.7037\n",
            "Epoch 3/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1204s\u001b[0m 15s/step - accuracy: 0.7484 - loss: 0.5133 - val_accuracy: 0.6250 - val_loss: 0.6303\n",
            "Epoch 4/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1204s\u001b[0m 15s/step - accuracy: 0.7652 - loss: 0.4921 - val_accuracy: 0.6250 - val_loss: 0.6438\n",
            "Epoch 5/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1200s\u001b[0m 15s/step - accuracy: 0.7678 - loss: 0.4899 - val_accuracy: 0.6250 - val_loss: 0.6625\n",
            "Epoch 6/10\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1222s\u001b[0m 15s/step - accuracy: 0.7659 - loss: 0.4948 - val_accuracy: 0.5625 - val_loss: 0.6561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions from Stage 1 for Stage 2 input\n",
        "filenames = test_generator.filenames\n",
        "true_labels = test_generator.classes\n",
        "pred_probs = model_stage1.predict(test_generator)\n",
        "pred_labels = (pred_probs > 0.5).astype(int).flatten()"
      ],
      "metadata": {
        "id": "lOiMJxdrrQn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce357171-dc98-46da-f79f-9ea09359a712"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 14s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect pneumonia cases only for Stage 2\n",
        "pneumonia_indices = np.where(pred_labels == 1)[0]\n",
        "pneumonia_filenames = [filenames[i] for i in pneumonia_indices]\n",
        "pneumonia_preds = pred_probs[pneumonia_indices]"
      ],
      "metadata": {
        "id": "pVxPYFTFsCbS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Stage 2 Preparation: Bacterial vs Viral classification setup\n",
        "# Construct custom dataframe for Stage 2 filtering from original test set\n",
        "stage2_dir = '/content/stage2_filtered'\n",
        "if os.path.exists(stage2_dir):\n",
        "    shutil.rmtree(stage2_dir)\n",
        "os.makedirs(stage2_dir + '/bacteria', exist_ok=True)\n",
        "os.makedirs(stage2_dir + '/virus', exist_ok=True)"
      ],
      "metadata": {
        "id": "B7o1X-qfsFP2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy only pneumonia cases to new dir and separate by label\n",
        "for i in pneumonia_indices:\n",
        "    fname = filenames[i]\n",
        "    label = true_labels[i]\n",
        "    full_path = os.path.join(test_dir, fname)\n",
        "    if 'bacteria' in fname.lower():\n",
        "        shutil.copy(full_path, os.path.join(stage2_dir, 'bacteria', os.path.basename(fname)))\n",
        "    elif 'virus' in fname.lower():\n",
        "        shutil.copy(full_path, os.path.join(stage2_dir, 'virus', os.path.basename(fname)))\n"
      ],
      "metadata": {
        "id": "V4FzbtwVvYHc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 2 Data Generator\n",
        "stage2_datagen = ImageDataGenerator(rescale=1./255)\n",
        "stage2_generator = stage2_datagen.flow_from_directory(\n",
        "    stage2_dir,\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=1,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "eh2kaBpkvcwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41236372-ef3c-4f2e-ee3a-9e153db68ae0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 336 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Stage 2 Model: InceptionV3 for Bacterial vs Viral\n",
        "base_model_2 = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
        "\n",
        "y2 = base_model_2.output\n",
        "y2 = GlobalAveragePooling2D()(y2)\n",
        "y2 = Dropout(0.5)(y2)\n",
        "y2 = Dense(128, activation='relu')(y2)\n",
        "output_2 = Dense(1, activation='sigmoid')(y2)\n",
        "\n",
        "model_stage2 = Model(inputs=base_model_2.input, outputs=output_2)\n",
        "\n",
        "for layer in base_model_2.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_stage2.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "EAsxNEdpvfTi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b51e60-fcc8-4a8b-96fe-6d4c93d4eda2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Stage 2\n",
        "history_stage2 = model_stage2.fit(\n",
        "    stage2_generator,\n",
        "    epochs=10,\n",
        "    callbacks=[EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "id": "LCncXayovlTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc908638-7b19-4e27-b609-208c5679242d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m336/336\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 201ms/step - accuracy: 0.6221 - loss: 5.5282\n",
            "Epoch 2/10\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 201ms/step - accuracy: 0.6039 - loss: 6.3847\n",
            "Epoch 3/10\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 198ms/step - accuracy: 0.6280 - loss: 5.9957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on Stage 2 filtered samples\n",
        "stage2_probs = model_stage2.predict(stage2_generator)\n",
        "stage2_preds = (stage2_probs > 0.5).astype(int).flatten()\n"
      ],
      "metadata": {
        "id": "H-W4cc8avntY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b089f5-74da-4532-c604-983a2ba2866a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m336/336\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 194ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Diagnosis and Treatment Recommendations\n",
        "# 0: Bacteria ‚Üí Antibiotics recommended\n",
        "# 1: Virus ‚Üí Supportive treatment only\n",
        "\n",
        "diagnosis_report = []\n",
        "for fname, pred in zip(stage2_generator.filenames, stage2_preds):\n",
        "    if pred == 0:\n",
        "        diagnosis = \"Bacterial Pneumonia\"\n",
        "        treatment = \"Prescribe antibiotics such as Azithromycin or Amoxicillin.\"\n",
        "    else:\n",
        "        diagnosis = \"Viral Pneumonia\"\n",
        "        treatment = \"Provide supportive care. Antivirals if confirmed and early.\"\n",
        "    diagnosis_report.append({\n",
        "        'Filename': fname,\n",
        "        'Diagnosis': diagnosis,\n",
        "        'Treatment': treatment\n",
        "    })\n"
      ],
      "metadata": {
        "id": "oSrpob6UvrWg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save report\n",
        "diagnosis_df = pd.DataFrame(diagnosis_report)\n",
        "diagnosis_df.to_csv(\"diagnosis_treatment_stage2.csv\", index=False)\n",
        "print(\"\\n‚úÖ Diagnosis and treatment report saved to diagnosis_treatment_stage2.csv\")\n"
      ],
      "metadata": {
        "id": "DX6QqD8ZvuOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0929abe8-b944-47f8-f211-0fdf5730b7ed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Diagnosis and treatment report saved to diagnosis_treatment_stage2.csv\n"
          ]
        }
      ]
    }
  ]
}